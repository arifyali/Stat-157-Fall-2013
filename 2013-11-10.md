This week was great because the data curators were able to finish the major project that we had been working on which was cleaning and caching the data.

I was part of the caching subgroup, named Cache Money, and we explored various ways to cache the earthquake data. I had suggested using Google Spreadsheet because I believed that it would be better than the CSV route. This was due to learning from Alisha that the current CSV was too big to push to GitHub. After ticketing with Gspread, I found that each spreadsheet was limited to only 400,000 cells, meaning the CSV was the only possible route. I found that git add. had an -f which would ignore githubs recommended file size and allow me to push the CSV with I did. however, Tristan figured out a more efficient storage method, which resulted in the obsolete need of a large CSV file.

We looked at other goals to solve which involved formatting changes and located other sources. The later, we believe to be un-needed because we have a cache of over 600,000 earthquakes.

While we have a lot of data, the analyzes ran into an efficiency problem. Their code would require two days to run, so we were asked to create a small sappy of earthquakes to test the code. While we are doing that, I think that code efficiency might be something to think about because two days seems way too long.

Since the main tasks of data curators are, we are attaching ourselves to analyzes and visualizers as data support.
